{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Foruck/sentiment-analysis-demo/blob/attention/bert_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6r0ZwikpHmYO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 准备工作"
      ]
    },
    {
      "metadata": {
        "id": "fxZi1hp_GZbK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 加载数据\n",
        "\n",
        "上传\n",
        "cn_train_data.h5\n",
        "en_train_data.h5\n",
        "cn_valid_data.h5\n",
        "en_valid_data.h5\n",
        "到自己的Google Drive\n"
      ]
    },
    {
      "metadata": {
        "id": "WVGVxS45EPaQ",
        "colab_type": "code",
        "outputId": "e1bccf75-fcb6-48c3-dbfa-bf95b4606e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o3qrbzI6Gtuy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 配置文件目录环境\n",
        "\n",
        "log：训练日志目录\n",
        "\n",
        "bst_model：最优checkpoint\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GAygoCb1KJLS",
        "colab_type": "code",
        "outputId": "da316e3e-bb21-4c8b-fa1b-949875f0d91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install psmisc\n",
        "!rm -rf log\n",
        "!rm -rf bst_model\n",
        "!mkdir log\n",
        "!mkdir bst_model\n",
        "!cp /content/gdrive/My\\ Drive/cn_train_data.h5 cn_train_data.h5\n",
        "!cp /content/gdrive/My\\ Drive/en_train_data.h5 en_train_data.h5\n",
        "!cp /content/gdrive/My\\ Drive/cn_valid_data.h5 cn_valid_data.h5\n",
        "!cp /content/gdrive/My\\ Drive/en_valid_data.h5 en_valid_data.h5\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  psmisc\n",
            "0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 51.5 kB of archives.\n",
            "After this operation, 254 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 psmisc amd64 23.1-1 [51.5 kB]\n",
            "Fetched 51.5 kB in 0s (127 kB/s)\n",
            "Selecting previously unselected package psmisc.\n",
            "(Reading database ... 110842 files and directories currently installed.)\n",
            "Preparing to unpack .../psmisc_23.1-1_amd64.deb ...\n",
            "Unpacking psmisc (23.1-1) ...\n",
            "Setting up psmisc (23.1-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "bst_model\t  cn_valid_data.h5  en_valid_data.h5  log\n",
            "cn_train_data.h5  en_train_data.h5  gdrive\t      sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wdc5tA4dHKJX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 配置python运行环境"
      ]
    },
    {
      "metadata": {
        "id": "C4vjhWx9R0CV",
        "colab_type": "code",
        "outputId": "3cfb1ef1-b380-4d40-b963-5a44c8e1ac3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        }
      },
      "cell_type": "code",
      "source": [
        "!pip uninstall torch\n",
        "!pip uninstall torchvision\n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install torch torchvision\n",
        "import torch\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n",
        "\n",
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mSkipping torch as it is not installed.\u001b[0m\n",
            "\u001b[33mSkipping torchvision as it is not installed.\u001b[0m\n",
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 25kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x62616000 @  0x7f91dea222a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 19.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 3.9MB/s \n",
            "\u001b[?25hInstalling collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-1.0.0 torchvision-0.2.1\n",
            "Torch 1.0.0 CUDA 9.0.176\n",
            "Device: cuda:0\n",
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/68/84de54aea460eb5b2e90bf47a429aacc1ce97ff052ec40874ea38ae2331d/pytorch_pretrained_bert-0.4.0-py3-none-any.whl (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.67)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.18.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.6)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.0.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.3)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.1.13)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.67 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.67)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.22)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.67->boto3->pytorch_pretrained_bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.67->boto3->pytorch_pretrained_bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.67->boto3->pytorch_pretrained_bert) (1.11.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fnVUgQZbHeEY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 开始"
      ]
    },
    {
      "metadata": {
        "id": "u6ZxFBtmHsXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 导入模块"
      ]
    },
    {
      "metadata": {
        "id": "1up16TvMNwHU",
        "colab_type": "code",
        "outputId": "4d1b2ec9-c8e1-44fc-9330-524ba4198d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import datetime\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import h5py\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertAdam"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S_Cw9G6MHxI3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 模型超参数设置\n",
        "\n",
        "embedding_length\n",
        "Sentence_Max_Length\n",
        "不可改动"
      ]
    },
    {
      "metadata": {
        "id": "rPzTLw2iNxoL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LSTM_Hidden_Size = 256\n",
        "embedding_length = 768\n",
        "Sentence_Max_Length = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmcepKJbH81_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 模型"
      ]
    },
    {
      "metadata": {
        "id": "MqvH0ISrEQi_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class bertDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data_path):\n",
        "    with h5py.File(data_path, 'r') as f:\n",
        "      self.data = f['data'][:, :]\n",
        "      self.mask = f['mask'][:, :]\n",
        "      self.annot = f['annot'][:]\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.annot.shape[0]\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx, :], self.mask[idx, :], self.annot[idx]\n",
        "\n",
        "      \n",
        "class myBert(torch.nn.Module):\n",
        "  def __init__(self, embedding_length=768, bert_path='bert-base-uncased', window=[7], classes=2, use_cuda=True):\n",
        "    super(myBert, self).__init__()\n",
        "    self.use_cuda = use_cuda\n",
        "    self.num_filter = len(window)\n",
        "\n",
        "    # Bert model\n",
        "    self.bert = BertModel.from_pretrained(bert_path)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    # LSTM layers\n",
        "    self.lstm1 = nn.LSTMCell(embedding_length, LSTM_Hidden_Size) # out n*1*LSTM_Hidden_Size\n",
        "    self.lstm2 = nn.LSTMCell(embedding_length, LSTM_Hidden_Size)\n",
        "    \n",
        "    # Attention\n",
        "    self.h1k1 = nn.Linear(LSTM_Hidden_Size, 64)\n",
        "    self.k1e1 = nn.Linear(64, 1)\n",
        "    self.h2k2 = nn.Linear(LSTM_Hidden_Size, 64)\n",
        "    self.k2e2 = nn.Linear(64, 1)\n",
        "\n",
        "    # FC layer\n",
        "    self.fc = nn.Linear(LSTM_Hidden_Size * 2, classes)\n",
        "    init.kaiming_normal_(self.fc.weight.data)\n",
        "    self.fc.bias.data.fill_(0)\n",
        "    \n",
        "    self.simple_fc = nn.Linear(768, 2)\n",
        "    \n",
        "  def forward(self, inputs, mask):\n",
        "    # Get Features\n",
        "    inputs = self.bert(inputs, token_type_ids=None, attention_mask=mask, output_all_encoded_layers=False)[0]\n",
        "    inputs = inputs.unsqueeze(1)\n",
        "        \n",
        "    # Go through Bi-LSTM\n",
        "    n = inputs.shape[0]\n",
        "    x0 = inputs.squeeze(1)\n",
        "    if self.use_cuda:\n",
        "      if not inputs.is_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "        x0 = x0.cuda()\n",
        "      cx1 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hx1 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hx2 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      cx2 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hxs1 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size)).cuda()\n",
        "      hxs2 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size)).cuda()\n",
        "    else:\n",
        "      if inputs.is_cuda:\n",
        "        inputs = inputs.cpu()\n",
        "        x0 = x0.cpu()\n",
        "      cx1 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hx1 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      cx2 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hx2 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hxs1 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size))\n",
        "      hxs2 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size))\n",
        "    \n",
        "    for i in range(x0.shape[1]):\n",
        "      hx1, cx1 = self.lstm1(x0[:, i, :], (hx1, cx1))\n",
        "      hxs1[:, i, :] = hx1\n",
        "      hx2, cx2 = self.lstm1(x0[:, x0.shape[1] - 1 - i, :], (hx2, cx2))\n",
        "      hxs2[:, i, :] = hx2\n",
        "      \n",
        "    k1 = self.h1k1(hxs1) # n*128*64\n",
        "    e1 = self.k1e1(k1).squeeze(2) # n*128*1 -> n*128\n",
        "    e1 = F.softmax(e1, dim=1).unsqueeze(1) # n*1*128\n",
        "    lstm1 = torch.matmul(e1, hxs1).squeeze(1)\n",
        "    k2 = self.h2k2(hxs2) # n*128*64\n",
        "    e2 = self.k2e2(k2).squeeze(2)\n",
        "    e2 = F.softmax(e2, dim=1).unsqueeze(1)\n",
        "    lstm2 = torch.matmul(e2, hxs2).squeeze(1)\n",
        "    \n",
        "    lstm_x = torch.cat((lstm1, lstm2), 1)\n",
        "    \n",
        "    x1 = self.dropout(self.fc(lstm_x))\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RameSu2AIAf3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 训练与测试函数"
      ]
    },
    {
      "metadata": {
        "id": "SUqQ3pTgN_1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, logger, epoch=0, print_every=100):\n",
        "  model = model.train()\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  all_loss, all_accuracy = 0.0, 0.0\n",
        "  hit, cnt = 0, 0\n",
        "  for i, (x, mask, target) in enumerate(train_loader):\n",
        "    x = x.cuda().long()\n",
        "    mask = mask.cuda().long()\n",
        "    target = target.cuda().long()\n",
        "    target = torch.clamp(target, min=0, max=1)\n",
        "    \n",
        "    scores = model(x, mask)\n",
        "    loss = loss_fn(scores, target)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    pred = torch.argmax(scores, dim=1)\n",
        "    hit = torch.sum(target == pred)\n",
        "    accuracy = float(hit) / int(x.shape[0])\n",
        "    if i % print_every == 0:\n",
        "      logger.info('Epoch %d, Iter %d, loss=%.4f, acc=%.4f' % (epoch, i, loss, accuracy))\n",
        "      print(time.strftime(\"%Y-%m-%d %H:%M:%S \", time.localtime()) + 'Epoch %d, Iter %d, loss=%.4f, acc=%.4f' % (epoch, i, loss, accuracy))\n",
        "    \n",
        "    all_loss += float(loss) * int(x.shape[0])\n",
        "    all_accuracy += float(hit)\n",
        "    cnt += int(x.shape[0])\n",
        "    \n",
        "  all_loss /= float(cnt)\n",
        "  all_accuracy /= float(cnt)\n",
        "  logger.info('Epoch %d, train_loss=%.4f, train_accuracy=%.4f' % (epoch, all_loss, all_accuracy))\n",
        "  print(time.strftime(\"%Y-%m-%d %H:%M:%S \", time.localtime()) + 'Epoch %d, train_loss=%.4f, train_accuracy=%.4f' % (epoch, all_loss, all_accuracy))\n",
        "  return model, optimizer\n",
        "\n",
        "def evaluate(model, valid_loader, logger, epoch=0, print_every=100):\n",
        "  model = model.eval()\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  all_loss = 0.0\n",
        "  hit, tot = 0, 0\n",
        "  for i, (x, mask, target) in enumerate(valid_loader):\n",
        "    x = x.cuda().long()\n",
        "    mask = mask.cuda().long()\n",
        "    target = target.cuda().long()\n",
        "    target = torch.clamp(target, min=0, max=1)\n",
        "    \n",
        "    scores = model(x, mask)\n",
        "    loss = loss_fn(scores, target)\n",
        "    pred = torch.argmax(scores, dim=1)\n",
        "    \n",
        "    hit += float(torch.sum(target == pred))\n",
        "    all_loss += float(loss) * int(x.shape[0])\n",
        "    tot += int(x.shape[0])\n",
        "  \n",
        "  all_loss /= tot\n",
        "  accuracy = float(hit) / tot\n",
        "  logger.info('Epoch %d, valid_loss=%.4f, valid_accuracy=%.4f' % (epoch, all_loss, accuracy))\n",
        "  print(time.strftime(\"%Y-%m-%d %H:%M:%S \", time.localtime()) + 'Epoch %d, valid_loss=%.4f, valid_accuracy=%.4f' % (epoch, all_loss, accuracy))\n",
        "  return model, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dMEkaDU8IEhf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 查看显存使用状况"
      ]
    },
    {
      "metadata": {
        "id": "DZY_IjiVO4Zt",
        "colab_type": "code",
        "outputId": "34c1be43-69b2-4e90-b1bc-3c651e2ee0e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Dec 23 04:48:28 2018       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4iMOZxY8IHmo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 训练参数设置"
      ]
    },
    {
      "metadata": {
        "id": "hqUNGt4SOTc7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "device = torch.device(\"cuda\")\n",
        "end_epoch = 30\n",
        "tag = 'en' # en英文，cn中文\n",
        "lr = 5e-5 #可以调节\n",
        "checkpoint = ''\n",
        "bz = 24\n",
        "warmup_proportion = 0.1\n",
        "bst_acc = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Q6V9yWkIKi0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 数据集、模型加载"
      ]
    },
    {
      "metadata": {
        "id": "4tvUPCngPTQL",
        "colab_type": "code",
        "outputId": "06d2b3e9-66b9-4b2f-b23f-205a876efc16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "if tag == 'cn':\n",
        "  train_set = bertDataset('cn_train_data.h5')\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=bz, shuffle=True, num_workers=8)\n",
        "  valid_set = bertDataset('cn_valid_data.h5')\n",
        "  valid_loader = torch.utils.data.DataLoader(dataset=valid_set, batch_size=int(bz / 2), shuffle=True, num_workers=8)\n",
        "  myModel = myBert(embedding_length, 'bert-base-chinese', use_cuda=True)\n",
        "elif tag == 'en':\n",
        "  train_set = bertDataset('en_train_data.h5')\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=bz, shuffle=True, num_workers=8)\n",
        "  valid_set = bertDataset('en_valid_data.h5')\n",
        "  valid_loader = torch.utils.data.DataLoader(dataset=valid_set, batch_size=int(bz / 2), shuffle=True, num_workers=8)\n",
        "  myModel = myBert(embedding_length, 'bert-base-uncased', use_cuda=True)\n",
        "  \n",
        "if checkpoint != '':\n",
        "  state = torch.load(args.checkpoint)\n",
        "  myModel.load_state_dict(state['model_state'])\n",
        "  optimizer.load_state_dict(state['optim_state'])\n",
        "  epoch = state['epoch']\n",
        "  bst_acc = state['acc']\n",
        "else:\n",
        "  epoch = 0\n",
        "\n",
        "myModel = myModel.cuda()\n",
        "ignored_params = list(map(id, myModel.bert.parameters()))\n",
        "base_params = filter(lambda p: id(p) not in ignored_params, myModel.parameters())\n",
        "optimizer_grouped_parameters = [{'params': base_params, 'weight_decay': 0.01}, {'params': myModel.bert.parameters(), 'weight_decay': 0.0, 'lr': 3e-5}]\n",
        "optimizer = BertAdam(optimizer_grouped_parameters, lr=lr, warmup=warmup_proportion, t_total=train_set.__len__())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:09<00:00, 44426999.31B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "gecyiAAgIN-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### log、checkpoint存储目录设置"
      ]
    },
    {
      "metadata": {
        "id": "7ZpYW4W1RAqs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "job_name =  '_'.join([tag, str(lr), datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')])\n",
        "save_path = 'bst_model/' + job_name + '.pth'\n",
        "log_path = 'log/' + job_name + '.log'\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(level = logging.INFO)\n",
        "handler = logging.FileHandler(log_path)\n",
        "handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)\n",
        "logger.info(\"Start print log\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HKtl0HAuIT6e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 开始训练"
      ]
    },
    {
      "metadata": {
        "id": "wAX-He7RRIxE",
        "colab_type": "code",
        "outputId": "b2de727b-73f1-451d-d439-b5e7d83c39a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5957
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(epoch, end_epoch):\n",
        "  logger.info('=> Epoch %d, lr = %0.6f <=' % (i, lr))\n",
        "  myModel, optimizer = train(myModel, train_loader, optimizer, logger, epoch=i, print_every=50)\n",
        "  myModel, accuracy = evaluate(myModel, valid_loader, logger, epoch=i)\n",
        "  \n",
        "  if (accuracy > bst_acc):\n",
        "    bst_acc = accuracy\n",
        "    state = {'model_state': myModel.state_dict(), 'epoch': i, 'optim_state': optimizer.state_dict(), 'acc': bst_acc}\n",
        "    torch.save(state, save_path)\n",
        "  \n",
        "logger.info(\"Finish\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-12-23 04:48:55 Epoch 0, Iter 0, loss=0.6893, acc=0.5417\n",
            "2018-12-23 04:50:15 Epoch 0, Iter 50, loss=0.6801, acc=0.6667\n",
            "2018-12-23 04:51:36 Epoch 0, Iter 100, loss=0.5945, acc=0.6667\n",
            "2018-12-23 04:52:56 Epoch 0, Iter 150, loss=0.6469, acc=0.7083\n",
            "2018-12-23 04:54:17 Epoch 0, Iter 200, loss=0.4016, acc=0.7917\n",
            "2018-12-23 04:55:37 Epoch 0, Iter 250, loss=0.4293, acc=0.7917\n",
            "2018-12-23 04:56:58 Epoch 0, Iter 300, loss=0.5338, acc=0.6667\n",
            "2018-12-23 04:58:18 Epoch 0, Iter 350, loss=0.4901, acc=0.7083\n",
            "2018-12-23 04:59:39 Epoch 0, Iter 400, loss=0.3833, acc=0.8333\n",
            "2018-12-23 05:00:52 Epoch 0, train_loss=0.5375, train_accuracy=0.6919\n",
            "2018-12-23 05:01:58 Epoch 0, valid_loss=0.3475, valid_accuracy=0.8442\n",
            "2018-12-23 05:02:08 Epoch 1, Iter 0, loss=0.3622, acc=0.7917\n",
            "2018-12-23 05:03:28 Epoch 1, Iter 50, loss=0.2847, acc=0.8750\n",
            "2018-12-23 05:04:49 Epoch 1, Iter 100, loss=0.5175, acc=0.7083\n",
            "2018-12-23 05:06:10 Epoch 1, Iter 150, loss=0.3081, acc=0.8333\n",
            "2018-12-23 05:07:31 Epoch 1, Iter 200, loss=0.5292, acc=0.7083\n",
            "2018-12-23 05:08:52 Epoch 1, Iter 250, loss=0.6305, acc=0.7083\n",
            "2018-12-23 05:10:12 Epoch 1, Iter 300, loss=0.2858, acc=0.9167\n",
            "2018-12-23 05:11:33 Epoch 1, Iter 350, loss=0.3270, acc=0.7500\n",
            "2018-12-23 05:12:53 Epoch 1, Iter 400, loss=0.5505, acc=0.6667\n",
            "2018-12-23 05:14:06 Epoch 1, train_loss=0.4169, train_accuracy=0.7676\n",
            "2018-12-23 05:15:11 Epoch 1, valid_loss=0.3581, valid_accuracy=0.8368\n",
            "2018-12-23 05:15:13 Epoch 2, Iter 0, loss=0.5082, acc=0.5833\n",
            "2018-12-23 05:16:34 Epoch 2, Iter 50, loss=0.4185, acc=0.8333\n",
            "2018-12-23 05:17:54 Epoch 2, Iter 100, loss=0.4467, acc=0.7083\n",
            "2018-12-23 05:19:15 Epoch 2, Iter 150, loss=0.3039, acc=0.8333\n",
            "2018-12-23 05:20:35 Epoch 2, Iter 200, loss=0.5803, acc=0.7500\n",
            "2018-12-23 05:21:56 Epoch 2, Iter 250, loss=0.7584, acc=0.6667\n",
            "2018-12-23 05:23:16 Epoch 2, Iter 300, loss=0.2947, acc=0.7917\n",
            "2018-12-23 05:24:37 Epoch 2, Iter 350, loss=0.3216, acc=0.8750\n",
            "2018-12-23 05:25:57 Epoch 2, Iter 400, loss=0.3520, acc=0.6667\n",
            "2018-12-23 05:27:11 Epoch 2, train_loss=0.3548, train_accuracy=0.8007\n",
            "2018-12-23 05:28:15 Epoch 2, valid_loss=0.2619, valid_accuracy=0.8842\n",
            "2018-12-23 05:28:25 Epoch 3, Iter 0, loss=0.3407, acc=0.8750\n",
            "2018-12-23 05:29:46 Epoch 3, Iter 50, loss=0.2608, acc=0.7917\n",
            "2018-12-23 05:31:07 Epoch 3, Iter 100, loss=0.2960, acc=0.7500\n",
            "2018-12-23 05:32:27 Epoch 3, Iter 150, loss=0.2905, acc=0.7917\n",
            "2018-12-23 05:33:48 Epoch 3, Iter 200, loss=0.2592, acc=0.9167\n",
            "2018-12-23 05:35:08 Epoch 3, Iter 250, loss=0.3632, acc=0.7083\n",
            "2018-12-23 05:36:29 Epoch 3, Iter 300, loss=0.2841, acc=0.8333\n",
            "2018-12-23 05:37:49 Epoch 3, Iter 350, loss=0.2105, acc=0.8333\n",
            "2018-12-23 05:39:10 Epoch 3, Iter 400, loss=0.2393, acc=0.7917\n",
            "2018-12-23 05:40:23 Epoch 3, train_loss=0.2828, train_accuracy=0.8344\n",
            "2018-12-23 05:41:29 Epoch 3, valid_loss=0.3087, valid_accuracy=0.8827\n",
            "2018-12-23 05:41:31 Epoch 4, Iter 0, loss=0.2223, acc=0.9167\n",
            "2018-12-23 05:42:51 Epoch 4, Iter 50, loss=0.2347, acc=0.9167\n",
            "2018-12-23 05:44:12 Epoch 4, Iter 100, loss=0.2248, acc=0.8333\n",
            "2018-12-23 05:45:33 Epoch 4, Iter 150, loss=0.2363, acc=0.8750\n",
            "2018-12-23 05:46:53 Epoch 4, Iter 200, loss=0.2405, acc=0.8333\n",
            "2018-12-23 05:48:14 Epoch 4, Iter 250, loss=0.5592, acc=0.8750\n",
            "2018-12-23 05:49:35 Epoch 4, Iter 300, loss=0.3606, acc=0.8333\n",
            "2018-12-23 05:50:55 Epoch 4, Iter 350, loss=0.2035, acc=0.7917\n",
            "2018-12-23 05:52:16 Epoch 4, Iter 400, loss=0.2785, acc=0.7500\n",
            "2018-12-23 05:53:30 Epoch 4, train_loss=0.2328, train_accuracy=0.8505\n",
            "2018-12-23 05:54:35 Epoch 4, valid_loss=0.4094, valid_accuracy=0.8790\n",
            "2018-12-23 05:54:37 Epoch 5, Iter 0, loss=0.2046, acc=0.7917\n",
            "2018-12-23 05:55:58 Epoch 5, Iter 50, loss=0.1436, acc=0.9583\n",
            "2018-12-23 05:57:19 Epoch 5, Iter 100, loss=0.3476, acc=0.7917\n",
            "2018-12-23 05:58:39 Epoch 5, Iter 150, loss=0.2041, acc=0.8333\n",
            "2018-12-23 06:00:00 Epoch 5, Iter 200, loss=0.1508, acc=0.9167\n",
            "2018-12-23 06:01:21 Epoch 5, Iter 250, loss=0.1186, acc=1.0000\n",
            "2018-12-23 06:02:42 Epoch 5, Iter 300, loss=0.1068, acc=0.9583\n",
            "2018-12-23 06:04:02 Epoch 5, Iter 350, loss=0.2054, acc=0.9167\n",
            "2018-12-23 06:05:23 Epoch 5, Iter 400, loss=0.2634, acc=0.9167\n",
            "2018-12-23 06:06:37 Epoch 5, train_loss=0.2057, train_accuracy=0.8659\n",
            "2018-12-23 06:07:42 Epoch 5, valid_loss=0.3431, valid_accuracy=0.8902\n",
            "2018-12-23 06:07:52 Epoch 6, Iter 0, loss=0.1851, acc=0.8750\n",
            "2018-12-23 06:09:13 Epoch 6, Iter 50, loss=0.2111, acc=0.8333\n",
            "2018-12-23 06:10:33 Epoch 6, Iter 100, loss=0.3848, acc=0.7500\n",
            "2018-12-23 06:11:54 Epoch 6, Iter 150, loss=0.2603, acc=0.8750\n",
            "2018-12-23 06:13:15 Epoch 6, Iter 200, loss=0.2674, acc=0.7917\n",
            "2018-12-23 06:14:36 Epoch 6, Iter 250, loss=0.2619, acc=0.8333\n",
            "2018-12-23 06:15:56 Epoch 6, Iter 300, loss=0.2213, acc=0.9583\n",
            "2018-12-23 06:17:17 Epoch 6, Iter 350, loss=0.2027, acc=0.9167\n",
            "2018-12-23 06:18:37 Epoch 6, Iter 400, loss=0.2026, acc=0.8750\n",
            "2018-12-23 06:19:51 Epoch 6, train_loss=0.1962, train_accuracy=0.8651\n",
            "2018-12-23 06:20:55 Epoch 6, valid_loss=0.4586, valid_accuracy=0.8835\n",
            "2018-12-23 06:20:57 Epoch 7, Iter 0, loss=0.2027, acc=0.8750\n",
            "2018-12-23 06:22:18 Epoch 7, Iter 50, loss=0.1740, acc=0.8750\n",
            "2018-12-23 06:23:38 Epoch 7, Iter 100, loss=0.2025, acc=0.7500\n",
            "2018-12-23 06:24:58 Epoch 7, Iter 150, loss=0.2892, acc=0.7500\n",
            "2018-12-23 06:26:19 Epoch 7, Iter 200, loss=0.1447, acc=0.9583\n",
            "2018-12-23 06:27:39 Epoch 7, Iter 250, loss=0.1454, acc=0.8333\n",
            "2018-12-23 06:29:00 Epoch 7, Iter 300, loss=0.2104, acc=0.8333\n",
            "2018-12-23 06:30:20 Epoch 7, Iter 350, loss=0.3179, acc=0.8333\n",
            "2018-12-23 06:31:41 Epoch 7, Iter 400, loss=0.1454, acc=0.8750\n",
            "2018-12-23 06:32:54 Epoch 7, train_loss=0.1887, train_accuracy=0.8727\n",
            "2018-12-23 06:33:59 Epoch 7, valid_loss=0.5252, valid_accuracy=0.8801\n",
            "2018-12-23 06:34:01 Epoch 8, Iter 0, loss=0.2051, acc=0.9583\n",
            "2018-12-23 06:35:21 Epoch 8, Iter 50, loss=0.2023, acc=0.8333\n",
            "2018-12-23 06:36:41 Epoch 8, Iter 100, loss=0.2608, acc=0.8750\n",
            "2018-12-23 06:38:01 Epoch 8, Iter 150, loss=0.1455, acc=0.9167\n",
            "2018-12-23 06:39:22 Epoch 8, Iter 200, loss=0.1447, acc=0.8333\n",
            "2018-12-23 06:40:42 Epoch 8, Iter 250, loss=0.3756, acc=0.7500\n",
            "2018-12-23 06:42:03 Epoch 8, Iter 300, loss=0.2312, acc=0.8750\n",
            "2018-12-23 06:43:23 Epoch 8, Iter 350, loss=0.1737, acc=0.8333\n",
            "2018-12-23 06:44:43 Epoch 8, Iter 400, loss=0.1173, acc=0.8750\n",
            "2018-12-23 06:45:57 Epoch 8, train_loss=0.1859, train_accuracy=0.8686\n",
            "2018-12-23 06:47:01 Epoch 8, valid_loss=0.5584, valid_accuracy=0.8827\n",
            "2018-12-23 06:47:03 Epoch 9, Iter 0, loss=0.1451, acc=0.9583\n",
            "2018-12-23 06:48:24 Epoch 9, Iter 50, loss=0.1162, acc=0.9583\n",
            "2018-12-23 06:49:44 Epoch 9, Iter 100, loss=0.1445, acc=1.0000\n",
            "2018-12-23 06:51:04 Epoch 9, Iter 150, loss=0.2311, acc=0.9167\n",
            "2018-12-23 06:52:25 Epoch 9, Iter 200, loss=0.2589, acc=0.8750\n",
            "2018-12-23 06:53:45 Epoch 9, Iter 250, loss=0.1745, acc=0.8750\n",
            "2018-12-23 06:55:05 Epoch 9, Iter 300, loss=0.2797, acc=0.8750\n",
            "2018-12-23 06:56:26 Epoch 9, Iter 350, loss=0.1743, acc=0.8750\n",
            "2018-12-23 06:57:46 Epoch 9, Iter 400, loss=0.0868, acc=0.9583\n",
            "2018-12-23 06:59:00 Epoch 9, train_loss=0.1885, train_accuracy=0.8679\n",
            "2018-12-23 07:00:04 Epoch 9, valid_loss=0.5425, valid_accuracy=0.8898\n",
            "2018-12-23 07:00:06 Epoch 10, Iter 0, loss=0.2317, acc=0.9583\n",
            "2018-12-23 07:01:27 Epoch 10, Iter 50, loss=0.1640, acc=0.8333\n",
            "2018-12-23 07:02:47 Epoch 10, Iter 100, loss=0.1450, acc=0.8750\n",
            "2018-12-23 07:04:08 Epoch 10, Iter 150, loss=0.1446, acc=0.9167\n",
            "2018-12-23 07:05:28 Epoch 10, Iter 200, loss=0.3179, acc=0.8333\n",
            "2018-12-23 07:06:48 Epoch 10, Iter 250, loss=0.1734, acc=0.8750\n",
            "2018-12-23 07:08:09 Epoch 10, Iter 300, loss=0.1787, acc=0.8333\n",
            "2018-12-23 07:09:29 Epoch 10, Iter 350, loss=0.2025, acc=0.8750\n",
            "2018-12-23 07:10:49 Epoch 10, Iter 400, loss=0.1679, acc=0.8750\n",
            "2018-12-23 07:12:03 Epoch 10, train_loss=0.1805, train_accuracy=0.8730\n",
            "2018-12-23 07:13:08 Epoch 10, valid_loss=0.5435, valid_accuracy=0.8962\n",
            "2018-12-23 07:13:18 Epoch 11, Iter 0, loss=0.2600, acc=0.7917\n",
            "2018-12-23 07:14:39 Epoch 11, Iter 50, loss=0.2416, acc=0.9167\n",
            "2018-12-23 07:15:59 Epoch 11, Iter 100, loss=0.1178, acc=0.9167\n",
            "2018-12-23 07:17:20 Epoch 11, Iter 150, loss=0.1159, acc=0.9583\n",
            "2018-12-23 07:18:41 Epoch 11, Iter 200, loss=0.2022, acc=0.8750\n",
            "2018-12-23 07:20:01 Epoch 11, Iter 250, loss=0.1156, acc=0.8750\n",
            "2018-12-23 07:21:22 Epoch 11, Iter 300, loss=0.1444, acc=0.9583\n",
            "2018-12-23 07:22:42 Epoch 11, Iter 350, loss=0.3177, acc=0.7083\n",
            "2018-12-23 07:24:03 Epoch 11, Iter 400, loss=0.2330, acc=0.7917\n",
            "2018-12-23 07:25:16 Epoch 11, train_loss=0.1843, train_accuracy=0.8684\n",
            "2018-12-23 07:26:21 Epoch 11, valid_loss=0.4780, valid_accuracy=0.8924\n",
            "2018-12-23 07:26:23 Epoch 12, Iter 0, loss=0.2038, acc=0.8750\n",
            "2018-12-23 07:27:44 Epoch 12, Iter 50, loss=0.0871, acc=0.9583\n",
            "2018-12-23 07:29:04 Epoch 12, Iter 100, loss=0.1736, acc=0.9167\n",
            "2018-12-23 07:30:25 Epoch 12, Iter 150, loss=0.1445, acc=0.8750\n",
            "2018-12-23 07:31:45 Epoch 12, Iter 200, loss=0.0581, acc=0.9167\n",
            "2018-12-23 07:33:06 Epoch 12, Iter 250, loss=0.1788, acc=0.8750\n",
            "2018-12-23 07:34:26 Epoch 12, Iter 300, loss=0.1451, acc=0.9167\n",
            "2018-12-23 07:35:47 Epoch 12, Iter 350, loss=0.1735, acc=0.9167\n",
            "2018-12-23 07:37:07 Epoch 12, Iter 400, loss=0.0891, acc=0.9167\n",
            "2018-12-23 07:38:20 Epoch 12, train_loss=0.1827, train_accuracy=0.8696\n",
            "2018-12-23 07:39:25 Epoch 12, valid_loss=0.5688, valid_accuracy=0.8980\n",
            "2018-12-23 07:39:35 Epoch 13, Iter 0, loss=0.2023, acc=0.8333\n",
            "2018-12-23 07:40:56 Epoch 13, Iter 50, loss=0.2889, acc=0.8333\n",
            "2018-12-23 07:42:17 Epoch 13, Iter 100, loss=0.3178, acc=0.6667\n",
            "2018-12-23 07:43:37 Epoch 13, Iter 150, loss=0.1157, acc=0.9583\n",
            "2018-12-23 07:44:58 Epoch 13, Iter 200, loss=0.1450, acc=0.8333\n",
            "2018-12-23 07:46:19 Epoch 13, Iter 250, loss=0.2314, acc=0.8750\n",
            "2018-12-23 07:47:40 Epoch 13, Iter 300, loss=0.1734, acc=0.9167\n",
            "2018-12-23 07:49:00 Epoch 13, Iter 350, loss=0.2023, acc=0.8750\n",
            "2018-12-23 07:50:21 Epoch 13, Iter 400, loss=0.1445, acc=0.8750\n",
            "2018-12-23 07:51:35 Epoch 13, train_loss=0.1792, train_accuracy=0.8734\n",
            "2018-12-23 07:52:40 Epoch 13, valid_loss=0.5818, valid_accuracy=0.8879\n",
            "2018-12-23 07:52:42 Epoch 14, Iter 0, loss=0.1446, acc=0.8750\n",
            "2018-12-23 07:54:03 Epoch 14, Iter 50, loss=0.1181, acc=0.9167\n",
            "2018-12-23 07:55:24 Epoch 14, Iter 100, loss=0.1156, acc=0.9583\n",
            "2018-12-23 07:56:45 Epoch 14, Iter 150, loss=0.1445, acc=0.9167\n",
            "2018-12-23 07:58:05 Epoch 14, Iter 200, loss=0.2895, acc=0.7917\n",
            "2018-12-23 07:59:26 Epoch 14, Iter 250, loss=0.1156, acc=1.0000\n",
            "2018-12-23 08:00:47 Epoch 14, Iter 300, loss=0.1156, acc=1.0000\n",
            "2018-12-23 08:02:08 Epoch 14, Iter 350, loss=0.2022, acc=0.8750\n",
            "2018-12-23 08:03:28 Epoch 14, Iter 400, loss=0.1446, acc=0.9583\n",
            "2018-12-23 08:04:42 Epoch 14, train_loss=0.1743, train_accuracy=0.8776\n",
            "2018-12-23 08:05:48 Epoch 14, valid_loss=0.6343, valid_accuracy=0.8842\n",
            "2018-12-23 08:05:50 Epoch 15, Iter 0, loss=0.2311, acc=0.7500\n",
            "2018-12-23 08:07:10 Epoch 15, Iter 50, loss=0.1156, acc=0.9167\n",
            "2018-12-23 08:08:31 Epoch 15, Iter 100, loss=0.1444, acc=0.7917\n",
            "2018-12-23 08:09:52 Epoch 15, Iter 150, loss=0.1156, acc=0.9167\n",
            "2018-12-23 08:11:13 Epoch 15, Iter 200, loss=0.2311, acc=0.7917\n",
            "2018-12-23 08:12:33 Epoch 15, Iter 250, loss=0.1733, acc=0.7917\n",
            "2018-12-23 08:13:54 Epoch 15, Iter 300, loss=0.1445, acc=0.8333\n",
            "2018-12-23 08:15:15 Epoch 15, Iter 350, loss=0.1445, acc=0.9167\n",
            "2018-12-23 08:16:35 Epoch 15, Iter 400, loss=0.2022, acc=0.9167\n",
            "2018-12-23 08:17:49 Epoch 15, train_loss=0.1743, train_accuracy=0.8748\n",
            "2018-12-23 08:18:54 Epoch 15, valid_loss=0.6437, valid_accuracy=0.8928\n",
            "2018-12-23 08:18:56 Epoch 16, Iter 0, loss=0.1769, acc=0.8333\n",
            "2018-12-23 08:20:17 Epoch 16, Iter 50, loss=0.2022, acc=0.8750\n",
            "2018-12-23 08:21:38 Epoch 16, Iter 100, loss=0.0867, acc=0.9583\n",
            "2018-12-23 08:22:58 Epoch 16, Iter 150, loss=0.1733, acc=0.9167\n",
            "2018-12-23 08:24:19 Epoch 16, Iter 200, loss=0.1444, acc=0.9167\n",
            "2018-12-23 08:25:40 Epoch 16, Iter 250, loss=0.0867, acc=0.9167\n",
            "2018-12-23 08:27:00 Epoch 16, Iter 300, loss=0.1160, acc=1.0000\n",
            "2018-12-23 08:28:21 Epoch 16, Iter 350, loss=0.1444, acc=0.9167\n",
            "2018-12-23 08:29:41 Epoch 16, Iter 400, loss=0.0970, acc=0.8750\n",
            "2018-12-23 08:30:55 Epoch 16, train_loss=0.1796, train_accuracy=0.8733\n",
            "2018-12-23 08:32:00 Epoch 16, valid_loss=0.7294, valid_accuracy=0.8917\n",
            "2018-12-23 08:32:02 Epoch 17, Iter 0, loss=0.3178, acc=0.6667\n",
            "2018-12-23 08:33:23 Epoch 17, Iter 50, loss=0.3466, acc=0.7500\n",
            "2018-12-23 08:34:43 Epoch 17, Iter 100, loss=0.1733, acc=0.9167\n",
            "2018-12-23 08:36:04 Epoch 17, Iter 150, loss=0.1168, acc=0.9583\n",
            "2018-12-23 08:37:25 Epoch 17, Iter 200, loss=0.1446, acc=1.0000\n",
            "2018-12-23 08:38:45 Epoch 17, Iter 250, loss=0.2633, acc=0.8333\n",
            "2018-12-23 08:40:06 Epoch 17, Iter 300, loss=0.1155, acc=0.8750\n",
            "2018-12-23 08:41:27 Epoch 17, Iter 350, loss=0.0867, acc=0.9583\n",
            "2018-12-23 08:42:47 Epoch 17, Iter 400, loss=0.1733, acc=0.8333\n",
            "2018-12-23 08:44:01 Epoch 17, train_loss=0.1799, train_accuracy=0.8745\n",
            "2018-12-23 08:45:06 Epoch 17, valid_loss=0.6835, valid_accuracy=0.8917\n",
            "2018-12-23 08:45:08 Epoch 18, Iter 0, loss=0.2022, acc=0.8750\n",
            "2018-12-23 08:46:29 Epoch 18, Iter 50, loss=0.2600, acc=0.7500\n",
            "2018-12-23 08:47:49 Epoch 18, Iter 100, loss=0.0289, acc=1.0000\n",
            "2018-12-23 08:49:10 Epoch 18, Iter 150, loss=0.1444, acc=0.8333\n",
            "2018-12-23 08:50:31 Epoch 18, Iter 200, loss=0.2022, acc=0.8750\n",
            "2018-12-23 08:51:51 Epoch 18, Iter 250, loss=0.0867, acc=0.9583\n",
            "2018-12-23 08:53:12 Epoch 18, Iter 300, loss=0.2888, acc=0.7500\n",
            "2018-12-23 08:54:33 Epoch 18, Iter 350, loss=0.1155, acc=0.9167\n",
            "2018-12-23 08:55:53 Epoch 18, Iter 400, loss=0.2022, acc=0.8750\n",
            "2018-12-23 08:57:07 Epoch 18, train_loss=0.1744, train_accuracy=0.8776\n",
            "2018-12-23 08:58:12 Epoch 18, valid_loss=0.7847, valid_accuracy=0.8950\n",
            "2018-12-23 08:58:14 Epoch 19, Iter 0, loss=0.1444, acc=0.8333\n",
            "2018-12-23 08:59:35 Epoch 19, Iter 50, loss=0.1876, acc=0.8750\n",
            "2018-12-23 09:00:55 Epoch 19, Iter 100, loss=0.2311, acc=0.8333\n",
            "2018-12-23 09:02:16 Epoch 19, Iter 150, loss=0.1629, acc=0.9167\n",
            "2018-12-23 09:03:37 Epoch 19, Iter 200, loss=0.1156, acc=0.9167\n",
            "2018-12-23 09:04:57 Epoch 19, Iter 250, loss=0.1155, acc=0.8333\n",
            "2018-12-23 09:06:18 Epoch 19, Iter 300, loss=0.2022, acc=0.8333\n",
            "2018-12-23 09:07:39 Epoch 19, Iter 350, loss=0.1451, acc=0.8333\n",
            "2018-12-23 09:08:59 Epoch 19, Iter 400, loss=0.2022, acc=0.8750\n",
            "2018-12-23 09:10:13 Epoch 19, train_loss=0.1780, train_accuracy=0.8744\n",
            "2018-12-23 09:11:18 Epoch 19, valid_loss=0.7006, valid_accuracy=0.8984\n",
            "2018-12-23 09:11:29 Epoch 20, Iter 0, loss=0.1444, acc=0.8750\n",
            "2018-12-23 09:12:50 Epoch 20, Iter 50, loss=0.0867, acc=0.8750\n",
            "2018-12-23 09:14:10 Epoch 20, Iter 100, loss=0.1733, acc=0.8750\n",
            "2018-12-23 09:15:31 Epoch 20, Iter 150, loss=0.1155, acc=0.8750\n",
            "2018-12-23 09:16:52 Epoch 20, Iter 200, loss=0.2888, acc=0.8333\n",
            "2018-12-23 09:18:12 Epoch 20, Iter 250, loss=0.1155, acc=0.9167\n",
            "2018-12-23 09:19:33 Epoch 20, Iter 300, loss=0.2311, acc=0.7917\n",
            "2018-12-23 09:20:54 Epoch 20, Iter 350, loss=0.2022, acc=0.8333\n",
            "2018-12-23 09:22:15 Epoch 20, Iter 400, loss=0.2602, acc=0.8333\n",
            "2018-12-23 09:23:28 Epoch 20, train_loss=0.1737, train_accuracy=0.8741\n",
            "2018-12-23 09:24:34 Epoch 20, valid_loss=0.7430, valid_accuracy=0.8976\n",
            "2018-12-23 09:24:36 Epoch 21, Iter 0, loss=0.2022, acc=0.9583\n",
            "2018-12-23 09:25:56 Epoch 21, Iter 50, loss=0.2022, acc=0.7500\n",
            "2018-12-23 09:27:17 Epoch 21, Iter 100, loss=0.2311, acc=0.7917\n",
            "2018-12-23 09:28:38 Epoch 21, Iter 150, loss=0.1444, acc=0.9167\n",
            "2018-12-23 09:29:58 Epoch 21, Iter 200, loss=0.1156, acc=0.8750\n",
            "2018-12-23 09:31:19 Epoch 21, Iter 250, loss=0.1733, acc=0.8750\n",
            "2018-12-23 09:32:39 Epoch 21, Iter 300, loss=0.2599, acc=0.8750\n",
            "2018-12-23 09:34:00 Epoch 21, Iter 350, loss=0.1733, acc=0.9167\n",
            "2018-12-23 09:35:21 Epoch 21, Iter 400, loss=0.1155, acc=0.8750\n",
            "2018-12-23 09:36:34 Epoch 21, train_loss=0.1780, train_accuracy=0.8751\n",
            "2018-12-23 09:37:40 Epoch 21, valid_loss=0.7704, valid_accuracy=0.8969\n",
            "2018-12-23 09:37:41 Epoch 22, Iter 0, loss=0.1444, acc=0.9167\n",
            "2018-12-23 09:39:02 Epoch 22, Iter 50, loss=0.1155, acc=0.8750\n",
            "2018-12-23 09:40:23 Epoch 22, Iter 100, loss=0.1444, acc=0.9583\n",
            "2018-12-23 09:41:43 Epoch 22, Iter 150, loss=0.1444, acc=0.8750\n",
            "2018-12-23 09:43:04 Epoch 22, Iter 200, loss=0.2599, acc=0.7917\n",
            "2018-12-23 09:44:25 Epoch 22, Iter 250, loss=0.0867, acc=0.8750\n",
            "2018-12-23 09:45:46 Epoch 22, Iter 300, loss=0.2022, acc=0.9167\n",
            "2018-12-23 09:47:06 Epoch 22, Iter 350, loss=0.2599, acc=0.7083\n",
            "2018-12-23 09:48:27 Epoch 22, Iter 400, loss=0.3177, acc=0.7083\n",
            "2018-12-23 09:49:40 Epoch 22, train_loss=0.1760, train_accuracy=0.8726\n",
            "2018-12-23 09:50:46 Epoch 22, valid_loss=0.7712, valid_accuracy=0.8980\n",
            "2018-12-23 09:50:48 Epoch 23, Iter 0, loss=0.0578, acc=1.0000\n",
            "2018-12-23 09:52:08 Epoch 23, Iter 50, loss=0.1444, acc=0.9167\n",
            "2018-12-23 09:53:29 Epoch 23, Iter 100, loss=0.0867, acc=0.9583\n",
            "2018-12-23 09:54:50 Epoch 23, Iter 150, loss=0.1155, acc=0.9583\n",
            "2018-12-23 09:56:10 Epoch 23, Iter 200, loss=0.1155, acc=0.8750\n",
            "2018-12-23 09:57:31 Epoch 23, Iter 250, loss=0.1733, acc=0.9167\n",
            "2018-12-23 09:58:52 Epoch 23, Iter 300, loss=0.1733, acc=0.9167\n",
            "2018-12-23 10:00:13 Epoch 23, Iter 350, loss=0.2022, acc=0.9583\n",
            "2018-12-23 10:01:33 Epoch 23, Iter 400, loss=0.1444, acc=0.8750\n",
            "2018-12-23 10:02:47 Epoch 23, train_loss=0.1768, train_accuracy=0.8764\n",
            "2018-12-23 10:03:52 Epoch 23, valid_loss=0.7908, valid_accuracy=0.8935\n",
            "2018-12-23 10:03:54 Epoch 24, Iter 0, loss=0.2239, acc=0.8333\n",
            "2018-12-23 10:05:15 Epoch 24, Iter 50, loss=0.0578, acc=0.9583\n",
            "2018-12-23 10:06:36 Epoch 24, Iter 100, loss=0.1155, acc=0.9167\n",
            "2018-12-23 10:07:56 Epoch 24, Iter 150, loss=0.1444, acc=0.8333\n",
            "2018-12-23 10:09:17 Epoch 24, Iter 200, loss=0.0868, acc=0.9167\n",
            "2018-12-23 10:10:38 Epoch 24, Iter 250, loss=0.2311, acc=0.7083\n",
            "2018-12-23 10:11:59 Epoch 24, Iter 300, loss=0.0578, acc=0.9583\n",
            "2018-12-23 10:13:19 Epoch 24, Iter 350, loss=0.0578, acc=0.9583\n",
            "2018-12-23 10:14:40 Epoch 24, Iter 400, loss=0.1733, acc=0.8750\n",
            "2018-12-23 10:15:54 Epoch 24, train_loss=0.1783, train_accuracy=0.8730\n",
            "2018-12-23 10:16:59 Epoch 24, valid_loss=1.2491, valid_accuracy=0.8476\n",
            "2018-12-23 10:17:01 Epoch 25, Iter 0, loss=0.2896, acc=0.8750\n",
            "2018-12-23 10:18:22 Epoch 25, Iter 50, loss=6.9629, acc=0.5417\n",
            "2018-12-23 10:19:42 Epoch 25, Iter 100, loss=6.7879, acc=0.5417\n",
            "2018-12-23 10:21:02 Epoch 25, Iter 150, loss=11.8271, acc=0.4583\n",
            "2018-12-23 10:22:23 Epoch 25, Iter 200, loss=8.5910, acc=0.6667\n",
            "2018-12-23 10:23:43 Epoch 25, Iter 250, loss=20.3207, acc=0.2917\n",
            "2018-12-23 10:25:03 Epoch 25, Iter 300, loss=10.1210, acc=0.5000\n",
            "2018-12-23 10:26:24 Epoch 25, Iter 350, loss=16.4561, acc=0.3333\n",
            "2018-12-23 10:27:44 Epoch 25, Iter 400, loss=14.7420, acc=0.5417\n",
            "2018-12-23 10:28:57 Epoch 25, train_loss=10.4886, train_accuracy=0.5109\n",
            "2018-12-23 10:30:03 Epoch 25, valid_loss=13.0582, valid_accuracy=0.4946\n",
            "2018-12-23 10:30:05 Epoch 26, Iter 0, loss=9.4990, acc=0.3750\n",
            "2018-12-23 10:31:25 Epoch 26, Iter 50, loss=19.5279, acc=0.3333\n",
            "2018-12-23 10:32:45 Epoch 26, Iter 100, loss=10.0323, acc=0.5000\n",
            "2018-12-23 10:34:06 Epoch 26, Iter 150, loss=9.2971, acc=0.7083\n",
            "2018-12-23 10:35:26 Epoch 26, Iter 200, loss=15.0809, acc=0.4583\n",
            "2018-12-23 10:36:46 Epoch 26, Iter 250, loss=15.4830, acc=0.2917\n",
            "2018-12-23 10:38:07 Epoch 26, Iter 300, loss=12.2519, acc=0.5417\n",
            "2018-12-23 10:39:27 Epoch 26, Iter 350, loss=16.9203, acc=0.3333\n",
            "2018-12-23 10:40:47 Epoch 26, Iter 400, loss=21.0162, acc=0.4167\n",
            "2018-12-23 10:42:01 Epoch 26, train_loss=14.0423, train_accuracy=0.5004\n",
            "2018-12-23 10:43:06 Epoch 26, valid_loss=15.1083, valid_accuracy=0.4946\n",
            "2018-12-23 10:43:08 Epoch 27, Iter 0, loss=16.3414, acc=0.5417\n",
            "2018-12-23 10:44:28 Epoch 27, Iter 50, loss=12.9171, acc=0.6250\n",
            "2018-12-23 10:45:49 Epoch 27, Iter 100, loss=17.9461, acc=0.5833\n",
            "2018-12-23 10:47:09 Epoch 27, Iter 150, loss=17.0878, acc=0.4583\n",
            "2018-12-23 10:48:29 Epoch 27, Iter 200, loss=18.6583, acc=0.4167\n",
            "2018-12-23 10:49:50 Epoch 27, Iter 250, loss=20.1589, acc=0.4167\n",
            "2018-12-23 10:51:10 Epoch 27, Iter 300, loss=18.0171, acc=0.5000\n",
            "2018-12-23 10:52:30 Epoch 27, Iter 350, loss=15.5598, acc=0.5000\n",
            "2018-12-23 10:53:51 Epoch 27, Iter 400, loss=17.1758, acc=0.4167\n",
            "2018-12-23 10:55:04 Epoch 27, train_loss=16.0457, train_accuracy=0.5020\n",
            "2018-12-23 10:56:10 Epoch 27, valid_loss=17.2574, valid_accuracy=0.4946\n",
            "2018-12-23 10:56:11 Epoch 28, Iter 0, loss=18.9090, acc=0.5417\n",
            "2018-12-23 10:57:32 Epoch 28, Iter 50, loss=11.6459, acc=0.5417\n",
            "2018-12-23 10:58:52 Epoch 28, Iter 100, loss=19.0933, acc=0.5417\n",
            "2018-12-23 11:00:12 Epoch 28, Iter 150, loss=13.6278, acc=0.5833\n",
            "2018-12-23 11:01:33 Epoch 28, Iter 200, loss=24.0945, acc=0.4167\n",
            "2018-12-23 11:02:53 Epoch 28, Iter 250, loss=8.1710, acc=0.4583\n",
            "2018-12-23 11:04:13 Epoch 28, Iter 300, loss=25.3378, acc=0.3750\n",
            "2018-12-23 11:05:33 Epoch 28, Iter 350, loss=16.1209, acc=0.5833\n",
            "2018-12-23 11:06:53 Epoch 28, Iter 400, loss=23.9733, acc=0.4167\n",
            "2018-12-23 11:08:06 Epoch 28, train_loss=18.2550, train_accuracy=0.5049\n",
            "2018-12-23 11:09:12 Epoch 28, valid_loss=19.7371, valid_accuracy=0.4946\n",
            "2018-12-23 11:09:14 Epoch 29, Iter 0, loss=16.6483, acc=0.5833\n",
            "2018-12-23 11:10:34 Epoch 29, Iter 50, loss=21.8047, acc=0.4583\n",
            "2018-12-23 11:11:54 Epoch 29, Iter 100, loss=16.7597, acc=0.5833\n",
            "2018-12-23 11:13:14 Epoch 29, Iter 150, loss=20.7166, acc=0.5417\n",
            "2018-12-23 11:14:34 Epoch 29, Iter 200, loss=19.2250, acc=0.7083\n",
            "2018-12-23 11:15:54 Epoch 29, Iter 250, loss=28.5634, acc=0.3750\n",
            "2018-12-23 11:17:14 Epoch 29, Iter 300, loss=25.1058, acc=0.4167\n",
            "2018-12-23 11:18:34 Epoch 29, Iter 350, loss=34.5427, acc=0.3750\n",
            "2018-12-23 11:19:54 Epoch 29, Iter 400, loss=29.6642, acc=0.4583\n",
            "2018-12-23 11:21:07 Epoch 29, train_loss=21.2918, train_accuracy=0.4928\n",
            "2018-12-23 11:22:13 Epoch 29, valid_loss=22.5908, valid_accuracy=0.4946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PyH2SbvbIWxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 自行添加：/bst_model、/log内文件保存到google drive或本地\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1GAoZvBEI_RN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TO DO\n",
        "# !cp bst_model/en_5e-05_2018-12-23_04-48-53.pth gdrive/My\\ Drive/en_5e-05_2018-12-23_04-48-53.pth"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}